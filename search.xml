<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CNN原理与实践</title>
    <url>/2021/10/17/CNN%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5.html</url>
    <content><![CDATA[<h2 id="原理详解">原理详解</h2>
<h3 id="卷积">卷积</h3>
<p>通过设置一个<strong>filter或者称为kernel</strong>，对输入的矩阵进行<strong>卷积</strong>，对应位置相乘。<strong>步长为1</strong>，矩阵两个维度均减小<strong>kernel维度-1</strong>.</p>
<p><strong>可以通过设计特定的filter，让它去跟图片做卷积，就可以识别出图片中的某些特征</strong><span id="more"></span></p>
<p><strong>CNN（convolutional neural network），主要就是通过一个个的filter，不断地提取特征，从局部的特征到总体的特征，从而进行图像识别等等功能。</strong>学习的目的就是去学习这些filter中的参数。</p>
<meta name="referrer" content="no-referrer"/>
<p><img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211008193054401-148638570.jpg" /></p>
<h3 id="padding填白">Padding填白</h3>
<p>经过卷积后，图像会越来越小，padding分为<strong>same</strong>和<strong>valid</strong>。</p>
<p>same 就是卷积前后大小不变。<strong>在卷积前</strong>，对图像增加一个padding填白，使大小不变。先要确定卷积核的大小才能确定填充大小。</p>
<p>valid 就是不经过任何填白。</p>
<p>但是用什么填充呢，一般采用0填充。零填充：在图片像素的最外层加上若干层0值，若一层，记做p =1。<strong>0在权重乘积和运算中对最终结果不造成影响，也就避免了图片增加了额外的干扰信息。</strong></p>
<figure>
<img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211008193714290-897558988.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="步长stride">步长stride</h3>
<p>就是卷积核的移动长度，每次向右移动一个步长。</p>
<h3 id="pooling池化">pooling池化</h3>
<p>有的地方也成为<strong>采样sampling</strong></p>
<p>池化的目的使提取一定区域的主要特征，<strong>减少参数数量，防止模型过拟合</strong>。</p>
<p>常见的pooling有<strong>maxpooling</strong>，取一定区域的最大值。取步长为2，取区域最大值。还有averagePooling</p>
<figure>
<img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211008194057122-600716143.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="多通道卷积">多通道卷积</h3>
<p>全面看的都是输入的二维矩阵，现在看多通道，即三维的输入。</p>
<p>注意到下图要求，卷积核的最后一个3就是<strong>通道channel</strong>数，必须相同。</p>
<p>每一个kernel进行卷积得到一个二维矩阵，通过激活函数ReLU后放在一起即可。</p>
<p>这里需要主要一下维度的变化，<strong>kernel的数量==结果的通道数，输入通道数==kernel通道数</strong>。</p>
<figure>
<img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211008194352219-2047578984.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211008200101676-1316336134.png" alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_height = (input_height - filter_height + <span class="number">2</span> * P)/S + <span class="number">1</span></span><br><span class="line">new_width = (input_width - filter_width + <span class="number">2</span> * P)/S + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="cnn网络结构">CNN网络结构</h3>
<p>X--&gt;CONV(relu)--&gt;MAXPOOL--&gt;CONV(relu)--&gt;FC(relu)--&gt;FC(softmax)--&gt;Y</p>
<p>扁平化 <strong>(height,width,channel)</strong>的数据压缩成长度为 <strong>height × width × channel</strong> 的一维数组，然后再与 <strong>FC层</strong>连接，<strong>这之后就跟普通的神经网络无异了</strong>。</p>
<figure>
<img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211008194727363-1650633100.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="cnn相对传统网络的优势">CNN相对传统网络的优势</h3>
<ul>
<li>参数共享机制。网络的参数大大减少，可以使用较少的参数训练出更好的模型。</li>
<li>连接的稀疏性 输出的图像的任何一个单元仅和图像的一部分有关。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">shape : N*C*H*W  ,  卷积核： H*W*</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211010095235308-136260343.png" alt="image-20211010095224357" /><figcaption aria-hidden="true">image-20211010095224357</figcaption>
</figure>
<h2 id="cnn代码实践">CNN代码实践</h2>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/42559190">【DL笔记6】从此明白了卷积神经网络（CNN） - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/qq_34714751/article/details/85610966">(14条消息) 使用PyTorch实现CNN_dongyangY的博客-CSDN博客</a></p>
]]></content>
      <categories>
        <category>deep_learning</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2021/10/17/SVM.html</url>
    <content><![CDATA[<h1 id="svm模型">SVM模型</h1>
<h2 id="简介">简介</h2>
<p><strong>支持向量机（support vector machines, SVM）</strong>是一种<strong>二分类模型</strong>，它的基本模型是定义在特征空间上的<strong>间隔最大的线性分类器</strong>，间隔最大使它有别于感知机；SVM还包括<strong>核技巧</strong>，这使它成为实质上的非线性分类器。</p>
<span id="more"></span>
<p>SVM的的学习策略就是间隔最大化，可形式化为一个求解<strong>凸二次规划</strong>的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。</p>
<meta name="referrer" content="no-referrer"/>
<h2 id="相关概念">相关概念</h2>
<p><strong>线性可分</strong>：二维空间上，两类点被一条直线完全分开叫做线性可分。</p>
<p><img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]" /> 和 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]" /> 是 n 维欧氏空间中的两个点集。如果存在 n 维向量 w 和实数 b，使得所有属于 <img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]" /> 的点 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]" /> 都有 <img src="https://www.zhihu.com/equation?tex=wx_i+%2B+b+%3E+0" alt="[公式]" /> ，而对于所有属于 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]" /> 的点 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]" /> 则有 <img src="https://www.zhihu.com/equation?tex=wx_j+%2B+b+%3C+0" alt="[公式]" /> ，则我们称 <img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]" /> 和 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]" /> 线性可分。</p>
<p><strong>最大间隔平面</strong>：从二维扩展到多维空间中时，将 <img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]" /> 和 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]" /> 完全正确地划分开的 <img src="https://www.zhihu.com/equation?tex=wx%2Bb%3D0" alt="[公式]" /> 就成了一个超平面。</p>
<p>为了使这个超平面更具<strong>鲁棒性</strong>，我们会去找最佳超平面，以最大间隔把两类样本分开的超平面，也称之为最大间隔超平面。</p>
<p><strong>支持向量</strong>：样本中距离超平面最近的一些点，这些点叫做支持向量。</p>
<p><img src="https://img2020.cnblogs.com/blog/2348945/202109/2348945-20210902194615509-1318363853.jpg" alt="img" style="zoom:67%;float: center;" ></p>
<h2 id="svm最优化问题"><strong>SVM最优化问题</strong></h2>
<p>超平面表示方法 <span class="math display">\[
w^{T} x+b=0
\]</span> 扩展到 <span class="math inline">\(n\)</span> 维空间后，点 <span class="math inline">\(x=\left(x_{1}, x_{2} \ldots x_{n}\right)\)</span> 到直线 <span class="math inline">\(w^{T} x+b=0\)</span> 的距离为: <span class="math display">\[
\frac{\left|w^{T} x+b\right|}{\|w\|}
\]</span> 其中 <span class="math inline">\(\|w\|=\sqrt{w_{1}^{2}+\ldots w_{n}^{2}}\)</span></p>
<p>支持向量到超平面的距离为 d，其他点到超平面的距离大于 d。</p>
<p>于是有下列公式： <span class="math display">\[
\begin{cases}\frac{w^{T} x+b}{\| w|| d} \geq 1 \quad y &amp; =1 \\ \frac{w^{T} x+b}{\|w\| d} \leq-1 &amp; y=-1\end{cases}
\]</span> <span class="math inline">\(\|w\| d\)</span> 是正数, 我们暂且令它为 1 （之所以令它等于 1, 是为了方便推导和优化，且这样做对目 标函数的优化没有影响），故:</p>
<p><strong>注</strong>：个人觉得是因为这个超平面对于一定的数据集是确定的，w是常数，d也是常熟，只是未知而已，但是是确定的，就像高中导数经常用的x0，去掉对优化无影响。 <span class="math display">\[
\left\{\begin{array}{l}
w^{T} x+b \geq 1 \quad y=1 \\
w^{T} x+b \leq-1 \quad y=-1
\end{array}\right.
\]</span> 将两个方程合并，我们可以简写为:</p>
<p><strong>注</strong>：还要加上y=1或-1的条件才行，参考中的这个有点问题 <span class="math display">\[
y\left(w^{T} x+b\right) \geq 1
\]</span> 至此我们就可以得到最大间隔超平面的上下两个超平面：</p>
<p><img src="https://img2020.cnblogs.com/blog/2348945/202109/2348945-20210902201557948-1924338082.jpg" alt="img" style="zoom:67%;float: center;" /></p>
<p>每个支持向量到超平面的距离可以写为: <span class="math display">\[
d=\frac{\left|w^{T} x+b\right|}{\|w\|}
\]</span> 由上述 <span class="math inline">\(y\left(w^{T} x+b\right)&gt;1&gt;0\)</span> 可以得到 <span class="math inline">\(y\left(w^{T} x+b\right)=\left|w^{T} x+b\right|\)</span>, 所以我们得到: <span class="math display">\[
d=\frac{y\left(w^{T} x+b\right)}{\|w\|}
\]</span> 这里乘上 2 倍也是为了后面推导，对目标函数没有影响。刚刚我们得到支持向量 <span class="math inline">\(y\left(w^{T} x+b\right)=1\)</span>, 所以我们得到: <span class="math display">\[
\max \frac{2}{\|w\|}
\]</span> 再做一个转换: <span class="math display">\[
\min \frac{1}{2}\|w\|
\]</span> 为了方便计算（去除 <span class="math inline">\(\|w\|\)</span> 的根号），我们有: <span class="math display">\[
\min \frac{1}{2}\|w\|^{2}
\]</span> 所以得到的<strong>最优化问题</strong>是：s.t.是subject to 局限于,加上y=+1或-1 <span class="math display">\[
\min \frac{1}{2}\|w\|^{2} 
s . t . \quad y_{i}\left(w^{T} x_{i}+b\right) \geq 1
\]</span> 这是一个含有不等式约束的凸二次规划问题，可以对其使用拉格朗日乘子法得到其对偶问题（dual problem）。</p>
<h2 id="对偶问题">对偶问题</h2>
<h3 id="拉格朗日乘数法">拉格朗日乘数法</h3>
<p>就是微积分多元函数求极值那个</p>
<h4 id="等式约束优化问题">等式约束优化问题</h4>
<p><span class="math display">\[
\min f\left(x_{1}, x_{2}, \ldots, x_{n}\right)
\]</span> s.t. <span class="math inline">\(\quad h_{k}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=0 \quad k=1,2, \ldots, l\)</span></p>
<p>我们令 <span class="math inline">\(L(x, \lambda)=f(x)+\sum_{k=1}^{l} \lambda_{k} h_{k}(x)\)</span>, 函数 <span class="math inline">\(L(x, y)\)</span> 称为 Lagrange 函数，参数 <span class="math inline">\(\lambda\)</span> 称 为 Lagrange 乘子没有非负要求。 利用必要条件找到可能的极值点: <span class="math display">\[
\begin{cases}\frac{\partial L}{\partial x_{i}}=0 &amp; i=1,2, \ldots, n \\ \frac{\partial L}{\partial \lambda_{k}}=0 &amp; k=1,2, \ldots, l\end{cases}
\]</span> 等式约束下的 Lagrange 乘数法引入了 <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]" /> 个 Lagrange 乘子，我们将 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]" /> 与 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D" alt="[公式]" /> 一视同仁，把 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D+" alt="[公式]" /> 也看作优化变量，共有 <img src="https://www.zhihu.com/equation?tex=%28n%2Bl%29" alt="[公式]" /> 个优化变量。</p>
<h4 id="不等式约束优化问题">不等式约束优化问题</h4>
<p>主要思想是将不等式约束条件转变为等式约束条件，引入<strong>松弛变量</strong>，将松弛变量也是为优化变量。</p>
<p>可以写为: <span class="math display">\[
\begin{array}{ll}
\underset{x}{\operatorname{minimize}} &amp; f(\boldsymbol{x}) \\
\text { subject to } &amp; g_{i}(\boldsymbol{x}) \leq 0, i=1,2, \cdots, m
\end{array}
\]</span> 引入拉格朗日乘子 <span class="math inline">\(\left(\mu_{i} \geq 0\right)\)</span>, 定义上述问题的拉格朗日量 (Lagrangian) 如下 <span class="math display">\[
L(x, \mu)=\left[f(x)+\sum_{i=1}^{m} \mu_{i} g_{i}(x)\right]
\]</span> 同时定义拉格朗日对偶函数 (Lagrange dual function) 如下: <span class="math display">\[
F(\mu)=i n f_{x} L(x, \mu)=i n f_{x}\left[f(x)+\sum_{i=1}^{m} \mu_{i} g_{i}(x)\right]
\]</span> 一般情况下, <span class="math inline">\(L(x, \mu)\)</span> 是能取到最小值的, 所以 <span class="math inline">\(F(\mu)=\inf _{x} L(x, \mu)=\min _{x} L(x, \mu)\)</span> 求解。当强对偶性成立时, 通过KKT条件求解极值点, 然后从极值点挑出最值点。</p>
<p>求解。当强对偶性成立时，通过KKT条件求解极值点，然后从极值点挑出最值点。 <span class="math display">\[
\left\{\begin{array}{l}
\nabla f(x)+\sum_{i=1}^{m} \mu_{i} \nabla g_{i}(x)=0 \\
g_{i}(x) \leq 0, \forall i=1, \cdots, m \\
\mu_{i} \geq 0, \forall i=1, \cdots, m \\
\mu_{i} g_{i}(x)=0, \forall j=1, \cdots, m
\end{array}\right.
\]</span> 第一个条件使得目标函数和约束函数的法向量共线（梯度共线）。 最后一个条件称为互补松弛条件(Complementary Slackness Condition)。通过引入这个条件, 增加了m个等式约束，使得等式的数量跟变量一样。</p>
<h3 id="强对偶性">强对偶性</h3>
<p>对偶问题其实就是将: <span class="math display">\[
\begin{gathered}
\min _{w} \max _{\lambda} L(w, \lambda) \\
s . t . \quad \lambda_{i} \geq 0
\end{gathered}
\]</span> 变成了： <span class="math display">\[
\begin{gathered}
\max _{\lambda} \min _{w} L(w, \lambda) \\
s . t . \quad \lambda_{i} \geq 0
\end{gathered}
\]</span> 类似于上面的倒数得到SVM最优化问题的表达式。</p>
<p>假设有个函数 <span class="math inline">\(f\)</span> 我们有: <span class="math display">\[
\min \max f \geq \max \min f
\]</span> 也就是说，最大的里面挑出来的最小的也要比最小的里面挑出来的最大的要大。这关系实际上就是 弱对偶关系，而强对偶关系是当等号成立时，即: <span class="math display">\[
\min \max f=\max \min f
\]</span> 如果 <span class="math inline">\(f\)</span> 是凸优化问题，强对偶性成立。</p>
<h2 id="svm优化过程">SVM优化过程</h2>
<p><strong>SVM优化公式</strong>： <span class="math display">\[
\begin{aligned}
  &amp;\quad \min _{w} \frac{1}{2}\|w\|^{2} \\
       &amp;\text { s.t. } \quad g_{i}(w, b)=1-y_{i} \quad\left(w^{T} x_{i}+b\right) \leq 0, \quad i=1,2, \ldots, n
\end{aligned}
\]</span></p>
<h3 id="构造拉格朗日函数">构造拉格朗日函数</h3>
<p>微积分多元函数学的，忘得差不多了。。，就是用来求最大最小值的 <span class="math display">\[
\begin{aligned}
\min _{w, b} \max _{\lambda} L(w, b, \lambda)=&amp; \frac{1}{2}\|w\|^{2}+\sum_{i=1}^{n} \lambda_{i}\left[1-y_{i}\left(w^{T} x_{i}+b\right)\right] \\
&amp; s . t . \quad \lambda_{i} \geq 0
\end{aligned}
\]</span></p>
<h3 id="利用强对偶性转化">利用强对偶性转化：</h3>
<p>利用强对偶性转化: <span class="math display">\[
\max _{\lambda} \min _{w, b} L(w, b, \lambda)
\]</span> 现对参数 <span class="math inline">\(\mathrm{w}\)</span> 和 <span class="math inline">\(\mathrm{b}\)</span> 求偏导数: <span class="math display">\[
\begin{aligned}
&amp;\frac{\partial L}{\partial w}=w-\sum_{i=1}^{n} \lambda_{i} x_{i} y_{i}=0 \\
&amp;\frac{\partial L}{\partial b}=\sum_{i=1}^{n} \lambda_{i} y_{i}=0
\end{aligned}
\]</span> 得到: <span class="math display">\[
\begin{aligned}
\sum_{i=1}^{n} \lambda_{i} x_{i} y_{i} &amp;=w \\
\sum_{i=1}^{n} \lambda_{i} y_{i} &amp;=0
\end{aligned}
\]</span> 我们将这个结果带回到函数中可得: <span class="math display">\[
\begin{aligned}
L(w, b, \lambda) &amp;=\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{n} \lambda_{i}-\sum_{i=1}^{n} \lambda_{i} y_{i}\left(\sum_{j=1}^{n} \lambda_{j} y_{j}\left(x_{i} \cdot x_{j}\right)+b\right) \\
&amp;=\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{n} \lambda_{i}-\sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{n} \lambda_{i} y_{i} b \\
&amp;=\sum_{i=1}^{n} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)
\end{aligned}
\]</span> 也就是说: <span class="math display">\[
\min _{w, b} L(w, b, \lambda)=\sum_{i=1}^{n} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)
\]</span> 去掉了w和b变量，得到只含有一个变量的式子，如下：</p>
<p><span class="math display">\[
\begin{gathered}
\max _{\lambda}\left[\sum_{i=1}^{n} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)\right] \\
\text { s.t. } \sum_{i=1}^{n} \lambda_{i} y_{i}=0 \quad \lambda_{i} \geq 0
\end{gathered}
\]</span> 我们可以看出来这是一个二次规划问题，问题规模正比于训练样本数，我们常用 <strong>SMO(Sequential Minimal Optimization) 算法</strong>求解。</p>
<p><strong>SMO(Sequential Minimal Optimization)，序列最小优化算法</strong>，其核心思想非常简单：每次只优 化一个参数, 其他参数先固定住, 仅求当前这个优化参数的极值。我们来看一下 SMO 算法在 SVM 中的应用。 我们刚说了 SMO 算法每次只优化一个参数，但我们的优化目标有约束条件: <span class="math inline">\(\sum_{i=1}^{n} \lambda_{i} y_{i}=0\)</span>,</p>
<ol type="1">
<li>选择两个需要更新的参数 <span class="math inline">\(\lambda_{i}\)</span> 和 <span class="math inline">\(\lambda_{j}\)</span>, 固定其他参数。于是我们有以下约束: 这样约束就变成了： <span class="math display">\[
\lambda_{i} y_{i}+\lambda_{j} y_{j}=c \quad \lambda_{i} \geq 0, \lambda_{j} \geq 0
\]</span> 其中 <span class="math inline">\(c=-\sum_{k \neq i, j} \lambda_{k} y_{k}\)</span>, 由此可以得出 <span class="math inline">\(\lambda_{j}=\frac{c-\lambda_{i} y_{i}}{y_{j}}\)</span>, 也就是说我们可以用 <span class="math inline">\(\lambda_{i}\)</span> 的表达 式代替 <span class="math inline">\(\lambda_{j}\)</span> 。这样就相当于把目标问题转化成了仅有一个约束条件的最优化问题, 仅有的约束是 <span class="math inline">\(\lambda_{i} \geq 0\)</span></li>
<li>对于仅有一个约束条件的最优化问题，我们完全可以在 <span class="math inline">\(\lambda_{i}\)</span> 上对优化目标求偏导, 令导数为 零, 从而求出变量值 <span class="math inline">\(\lambda_{i_{\text {new }}}\)</span>, 然后根据 <span class="math inline">\(\lambda_{i_{\text {new }}}\)</span> 求出 <span class="math inline">\(\lambda_{j_{\text {new }}}\)</span> 。</li>
<li>多次迭代直至收敛。 通过 <span class="math inline">\(\mathrm{SMO}\)</span> 求得最优解 <span class="math inline">\(\lambda^{*}\)</span> 。</li>
</ol>
<p>求偏导数时得到： <span class="math display">\[
w=\sum_{i=1}^{m} \lambda_{i} y_{i} x_{i}
\]</span> 由上式可求得 <span class="math inline">\(\mathrm{w}\)</span> 。 我们知道所有 <span class="math inline">\(\lambda_{i}&gt;0\)</span> 对应的点都是支持向量, 我们可以随便找个支持向量, 然后带入： <span class="math inline">\(y_{s}\left(w x_{s}+b\right)=1\)</span>, 求出 <span class="math inline">\(b\)</span> 即可, 两边同乘 <span class="math inline">\(y_{s}\)</span>, 得 <span class="math inline">\(y_{s}^{2}\left(w x_{s}+b\right)=y_{s}\)</span> 因为 <span class="math inline">\(y_{s}^{2}=1\)</span>, 所以: <span class="math inline">\(b=y_{s}-w x_{s}\)</span> 为了更具鲁棒性，我们可以求得支持向量的均值: <span class="math display">\[
b=\frac{1}{|S|} \sum_{s \in S}\left(y_{s}-w x_{s}\right)
\]</span> 步骤 <span class="math inline">\(5: \mathrm{w}\)</span> 和 <span class="math inline">\(\mathrm{b}\)</span> 都求出来了，我们就能构造出最大分割超平面： <span class="math inline">\(\boldsymbol{w}^{T} x+b=0\)</span> 分类决策函数： <span class="math inline">\(f(x)=\operatorname{sign}\left(w^{T} x+b\right)\)</span> 其中 <span class="math inline">\(\operatorname{sign}(\cdot)\)</span> 为阶跃函数： <span class="math display">\[
\operatorname{sign}(x)=\left\{\begin{array}{rl}
-1 &amp; x&lt;0 \\
0 &amp; x=0 \\
1 &amp; x&gt;0
\end{array}\right.
\]</span> 将新样本点导入到决策函数中既可得到样本的分类。</p>
<h2 id="核函数">核函数</h2>
<h4 id="问题">问题</h4>
<p>线性不可分问题：将二维线性不可分样本映射到高维空间中，让样本点在高维空间线性可分</p>
<p><img src="https://img2020.cnblogs.com/blog/2348945/202109/2348945-20210902213653815-1203370718.jpg" alt="img" style="zoom:67%;" /></p>
<h4 id="核函数意义">核函数意义</h4>
<p>这是因为低维空间映射到高维空间后维度可能会很大，如果将全部样本的点乘全部计算好，这样的 <strong>计算量太大</strong>了。 但如果我们有这样的一核函数 <span class="math inline">\(k(x, y)=(\phi(x), \phi(y)), \quad x_{i}\)</span> 与 <span class="math inline">\(x_{j}\)</span> 在特征空间的内积等于 它们在原始样本空间中通过函数 <span class="math inline">\(k(x, y)\)</span> 计算的结果，我们就不需要计算高维甚至无穷维空间的 内积了。</p>
<p>举个例子：假设我们有一个多项式核函数: <span class="math display">\[
k(x, y)=(x \cdot y+1)^{2}
\]</span> 带进样本点的后： <span class="math display">\[
k(x, y)=\left(\sum_{i=1}^{n}\left(x_{i} \cdot y_{i}\right)+1\right)^{2}
\]</span> 而它的展开项是： <span class="math display">\[
\sum_{i=1}^{n} x_{i}^{2} y_{i}^{2}+\sum_{i=2}^{n} \sum_{j=1}^{i-1}\left(\sqrt{2} x_{i} x_{j}\right)\left(\sqrt{2} y_{i} y_{j}\right)+\sum_{i=1} n\left(\sqrt{2} x_{i}\right)\left(\sqrt{2} y_{i}\right)+1
\]</span> 如果没有核函数，我们则需要把向量映射成: <span class="math display">\[
x^{\prime}=\left(x_{1}^{2}, \ldots, x_{n}^{2}, \ldots \sqrt{2} x_{1}, \ldots, \sqrt{2} x_{n}, 1\right)
\]</span> 然后在进行内积计算, 才能与多项式核函数达到相同的效果。</p>
<h4 id="常见核函数">常见核函数</h4>
<p>线性核函数 <span class="math display">\[
k\left(x_{i}, x_{j}\right)=x_{i}^{T} x_{j}
\]</span> 多项式核函数 <span class="math display">\[
k\left(x_{i}, x_{j}\right)=\left(x_{i}^{T} x_{j}\right)^{d}
\]</span> 高斯核函数 <span class="math display">\[
k\left(x_{i}, x_{j}\right)=\exp \left(-\frac{\left\|x_{i}-x_{j}\right\|}{2 \delta^{2}}\right)
\]</span></p>
<h2 id="使用sklearn实现">使用sklearn实现</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>) <span class="comment"># 使用相同的seed()值，则每次生成的随即数都相同</span></span><br><span class="line"><span class="comment"># 创建可线性分类的数据集与结果集</span></span><br><span class="line">X = np.r_[np.random.randn(<span class="number">20</span>, <span class="number">2</span>) - [<span class="number">2</span>, <span class="number">2</span>], np.random.randn(<span class="number">20</span>,<span class="number">2</span>) + [<span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">Y = [<span class="number">0</span>] * <span class="number">20</span> + [<span class="number">1</span>] * <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造 SVM 模型</span></span><br><span class="line">clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">clf.fit(X, Y) <span class="comment"># 训练 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#然后就是收集这些数据来画平面图，因为这是一个二维的模型，因此可以假设超平面方程为 w0x + w1y + b = 0 转为点斜式就是: y = -(w0/w1)x - (b/w1) ：</span></span><br><span class="line">w = clf.coef_[<span class="number">0</span>]</span><br><span class="line">a = -w[<span class="number">0</span>] / w[<span class="number">1</span>] <span class="comment"># 斜率</span></span><br><span class="line">xx = np.linspace(-<span class="number">5</span>, <span class="number">5</span>) <span class="comment"># 在区间[-5, 5] 中产生连续的值，用于画线</span></span><br><span class="line">yy = a * xx - (clf.intercept_[<span class="number">0</span>]) / w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">b = clf.support_vectors_[<span class="number">0</span>] <span class="comment"># 第一个分类的支持向量</span></span><br><span class="line">yy_down = a * xx + (b[<span class="number">1</span>] - a * b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">b = clf.support_vectors_[-<span class="number">1</span>] <span class="comment"># 第二个分类中的支持向量</span></span><br><span class="line">yy_up = a * xx + (b[<span class="number">1</span>] - a * b[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line"></span><br><span class="line">pl.plot(xx, yy, <span class="string">&#x27;k-&#x27;</span>)</span><br><span class="line">pl.plot(xx, yy_down, <span class="string">&#x27;k--&#x27;</span>)</span><br><span class="line">pl.plot(xx, yy_up, <span class="string">&#x27;k--&#x27;</span>)</span><br><span class="line">pl.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>],</span><br><span class="line">           s=<span class="number">80</span>, facecolors=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">pl.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, cmap=pl.cm.Paired)</span><br><span class="line"></span><br><span class="line">pl.axis(<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
<h3 id="优点">优点</h3>
<ul>
<li>有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题；</li>
<li>能找出对任务至关重要的关键样本（即：支持向量）；</li>
<li>采用核技巧之后，可以处理非线性分类/回归任务；</li>
<li>最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。</li>
</ul>
<h3 id="缺点">缺点</h3>
<ul>
<li>训练时间长。当采用 SMO 算法时，由于每次都需要挑选一对参数，因此时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28N%5E2%29" alt="[公式]" /> ，其中 N 为训练样本的数量；</li>
<li>当采用核技巧时，如果需要存储核矩阵，则空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28N%5E2%29" alt="[公式]" /> ；</li>
<li>模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。</li>
</ul>
<p>因此支持向量机目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务。</p>
<h2 id="参考">参考</h2>
<p>[1] <a href="https://zhuanlan.zhihu.com/p/77750026">【机器学习】支持向量机 SVM（非常详细） - 知乎 (zhihu.com)</a></p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/31886934">支持向量机（SVM）——原理篇 - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title>books</title>
    <url>/2021/10/19/books.html</url>
    <content><![CDATA[<figure>
<img src="../images/books/image-20211019110546295.png" alt="image-20211019110546295" /><figcaption aria-hidden="true">image-20211019110546295</figcaption>
</figure>
]]></content>
      <categories>
        <category>ctf</category>
      </categories>
      <tags>
        <tag>pwn</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2vec</title>
    <url>/2021/10/17/word2vec.html</url>
    <content><![CDATA[<h2 id="简介">简介</h2>
<p>Word2Vec是语言模型中的一种，它是从大量文本预料中以无监督方式学习语义知识的模型，被广泛地应用于自然语言处理中。</p>
<p>Word2Vec是用来生成词向量的工具，而词向量与语言模型有着密切的关系。</p>
<span id="more"></span>
<meta name="referrer" content="no-referrer"/>
<h2 id="基础概念">基础概念</h2>
<h3 id="语料">语料</h3>
<p>看word2vec之前可以先看<a href="https://www.cnblogs.com/dddddblog/p/n_gram.html">n-gram模型</a></p>
<p>word2vec是一个统计语言模型，通过计算条件概率来预测和生成向量。</p>
<p>n-gram模型在语料库足够大的情况下，不单独统计各个条件概率，而是直接使用这种大概的方式，减少计算</p>
<p>​ <span class="math display">\[p(w_k|w_{k-n+1},...,w_{k-1}) = \frac{count(w_{k-n},...,w_k)}{count(w_{k-n+1},...w_{k-1})}\]</span></p>
<p>参数n（word2vec中的窗口大小）确定需要根据模型参数的数量来确定。一般来说越大越好，后面效果增加减少。</p>
<p>对于统计语言模型而言，利用最大似然，可把目标函数设为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cprod_%7Bw+%5Cin+C%7D%5E%7B%7D+p%28w%7CContext%28w%29%29+%5C%5C" alt="[公式]" /> 其中，C表示语料(Corpus)，Context(w)表示词w的上下文，即w周边的词的集合。当Context(w)为空时，就取 <img src="https://www.zhihu.com/equation?tex=p%28w%7CContext%28w%29%29%3D+p%28w%29" alt="[公式]" /> 。特别地，对于前面介绍的n-gram模型，就有 <img src="https://www.zhihu.com/equation?tex=Context%28w_%7Bi%7D+%3D+w_%7Bi-n%2B1%7D%2C...%2Cw_%7Bi-1%7D%29" alt="[公式]" /> 。</p>
<h3 id="模型基础">模型基础</h3>
<p>当然，实际应用中常采用最大对数似然，即把目标函数设为</p>
<p><img src="https://www.zhihu.com/equation?tex=L+%3D+%5Csum_%7Bw+%5Cin+C%7D%5E%7B%7D%7Blog+p%28w%7CContext%28w%29%29%7D++%5Ctag%7B4%7D%5C%5C" alt="[公式]" /> 然后对这个函数进行最大化。</p>
<p>从公式（4）可见，概率 <img src="https://www.zhihu.com/equation?tex=p%28w%7CContext%28w%29%29" alt="[公式]" /> 已被视为关于w和Context(w)的函数，即：</p>
<p><img src="https://www.zhihu.com/equation?tex=+p%28w%7CContext%28w%29%29+%3D+F%28w%2C+Context%28w%29%2C+%5Ctheta%29+%5C%5C" alt="[公式]" /> 其中 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]" /> 为待定参数集。这样一来，一旦对（4）进行优化得到最优参数集 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B+%2A%7D" alt="[公式]" /> 后，F也就唯一被确定了，以后任何概率p(w|Context(w))就可以通过函数 <img src="https://www.zhihu.com/equation?tex=F%28w%2C+Context%28w%29%2C+%5Ctheta%5E%7B+%2A%7D%29" alt="[公式]" /> 来计算了。与n-gram相比，这种方法不需要事先计算并保存所有的概率值，而是通过直接计算来获取，且通选取合适的模型可使得 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]" /> 中参数的个数远小于n-gram中模型参数的个数。</p>
<h2 id="word2vec网络结构">word2vec网络结构</h2>
<p>word2vec是轻量级的神经网络，其模型仅仅包括输入层、隐藏层和输出层，分为CBOW和Skip-gram模型。CBOW是知道上下文语料的情况下，预测当前的词。skip-gram则是根据一个词预测上下文。上下文是指根据window_size设置的前后各n个词。</p>
<p><img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211013225352657-2105331470.png" /></p>
<h3 id="cbow模型">CBOW模型</h3>
<p><img src="https://pic3.zhimg.com/80/v2-55eadeeae1fb93907d9b3da9aabed576_720w.jpg" /></p>
<h3 id="skip-gram模型">skip-gram模型</h3>
<p><img src="https://pic4.zhimg.com/80/v2-3baa8ea48cf7028510871a2894ffbb97_720w.jpg" /></p>
<h2 id="word2vec实践">word2vec实践</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span>  <span class="comment"># 2 words to the left, 2 to the right</span></span><br><span class="line">raw_text = <span class="string">&quot;&quot;&quot;We are about to study the idea of a computational process.</span></span><br><span class="line"><span class="string">Computational processes are abstract beings that inhabit computers.</span></span><br><span class="line"><span class="string">As they evolve, processes manipulate other abstract things called data.</span></span><br><span class="line"><span class="string">The evolution of a process is directed by a pattern of rules</span></span><br><span class="line"><span class="string">called a program. People create programs to direct processes. In effect,</span></span><br><span class="line"><span class="string">we conjure the spirits of the computer with our spells.&quot;&quot;&quot;</span>.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># By deriving a set from `raw_text`, we deduplicate the array</span></span><br><span class="line">vocab = <span class="built_in">set</span>(raw_text)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line"><span class="built_in">print</span>(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view((<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span>(<span class="params">context, word_to_ix</span>):</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line">make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(<span class="built_in">len</span>(vocab), embedding_dim=<span class="number">10</span>, context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> data:</span><br><span class="line">        context_ids = make_context_vector(context, word_to_ix)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = torch.tensor([word_to_ix[target]], dtype=torch.long)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line"><span class="built_in">print</span>(losses)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.embeddings(make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix))</span><br></pre></td></tr></table></figure>
<h2 id="参考">参考</h2>
<p>[1] <a href="https://zhuanlan.zhihu.com/p/114538417">深入浅出Word2Vec原理解析 - 知乎 (zhihu.com)</a></p>
<p>[2] <a href="https://samaelchen.github.io/word2vec_pytorch/">word2vec的PyTorch实现 | 碎碎念 (samaelchen.github.io)</a></p>
]]></content>
      <categories>
        <category>deep_learning</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title>test_img</title>
    <url>/2021/10/17/test-img.html</url>
    <content><![CDATA[<p><img src="./test-img/image-20211017195615356.png" /></p>
<figure>
<img src="test-img/image-20211017200351123.png" alt="image-20211017200351123" /><figcaption aria-hidden="true">image-20211017200351123</figcaption>
</figure>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/10/17/hello-world.html</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<meta name="referrer" content="no-referrer"/>
<h2 id="quick-start">Quick Start</h2>
<p><span class="math display">\[hello sdsd(\frac{a}{b})\]</span></p>
<p><img src="/images/hello-world/image-20211017201247881.png" /></p>
<h3 id="create-a-new-post">Create a new post</h3>
<p><img src="https://img2020.cnblogs.com/blog/2348945/202110/2348945-20211017194040110-1069279989.png" /></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>time</category>
      </categories>
      <tags>
        <tag>pwn</tag>
      </tags>
  </entry>
  <entry>
    <title>RandomForest</title>
    <url>/2021/10/17/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.html</url>
    <content><![CDATA[<h2 id="决策树">决策树</h2>
<p>先来复习一下决策树，根据预测结果不同，决策树分为回归决策树和分类决策树。很多的决策树集成，就成为了森林。常见的算法有随机森林和GDBT。</p>
<span id="more"></span>
<meta name="referrer" content="no-referrer"/>
<p>决策树是一种树形结构每个内部分支都是基于一个属性，每个叶节点代表一种类型。其实决策树就是在将一个多维（特征个数）空间进行划分，切分，产生不同类别。</p>
<h3 id="分类决策树">分类决策树</h3>
<p>分类决策树本身就是通过计算信息增益，选择特征，根据特征的值的个数形成分支，剪支（还没学）。</p>
<p><span class="math inline">\(x_i\)</span>代表某个特征的取值，<span class="math inline">\(P(x_i)\)</span>代表该类型的占比，<strong>信息熵</strong>衡量的是该特征的<strong>混乱程度</strong>，公式：</p>
<p>​ <span class="math display">\[H(X)=-\sum_{i=1}^{n}p(x_i)log p(x_i)\]</span></p>
<p>决策树要做的就是通过对比使用每个特征进行分支产生的信息增益--信息熵的减少程度来选择特征。</p>
<p>参考：<a href="https://blog.csdn.net/Daycym/article/details/84455299">(12条消息) 【机器学习】分类决策树与回归决策树案例_Daycym的博客-CSDN博客</a></p>
<h3 id="回归决策树">回归决策树</h3>
<h3 id="classification-and-regression-tree-cart">（classification and regression tree, CART）</h3>
<p>这里复习一下<strong>回归和拟合的区别</strong>：</p>
<p>回归分析：是一种统计学上分析数据的方法，目的在于了解两个或多个变量间<strong>是否相关</strong>、<strong>相关方向与强度</strong>，并建立数学模型以便观察特定变量来预测研究者感兴趣的变量。</p>
<p>拟合：是一种把现有数据透过数学方法来代入<strong>一条数式</strong>的表示方式。</p>
<p>回归决策树将一个内部节点中的所有数据的该特征的平均值作为预测值。</p>
<p><img src="https://img2020.cnblogs.com/blog/2348945/202109/2348945-20210925133815973-632336899.png" alt="img" style="zoom: 50%;" /></p>
<figure>
<img src="https://img2020.cnblogs.com/blog/2348945/202109/2348945-20210925132930783-421450240.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="随机森林算法random-forestrf">随机森林算法（random forest,RF）</h2>
<h3 id="简介">简介</h3>
<p>随机森林是一种基于集成学习的思想和决策树模型而产生的分类器。<strong>集成学习（Ensemble Learning）</strong>通过使用多个弱分类器，来组成一个强分类器，提高准确率。使用多个不同决策树<strong>独立</strong>的决策结果，少数服从多数的方法来分类。</p>
<p>少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林<strong>bagging</strong>的思想。bagging的代价是不用单棵决策树来做预测，<strong>具体哪个变量起到重要作用变得未知</strong>，所以bagging<strong>改进了预测准确率但损失了解释性</strong>。</p>
<h3 id="决策树生成规则">决策树生成规则</h3>
<ol type="1">
<li>如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；（<strong>bootstrap sample取样方法</strong>，随机有放回）</li>
<li>如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；</li>
<li>每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
</ol>
<p><strong>解释</strong>：随机抽样可以让<strong>每个决策树使用的数据集不同</strong>，给出不同的预测结果。有放回不至于让每个决策树得到数据集中的一部分，这样的结果是<strong>片面的</strong>。</p>
<p>两个随机：随机取样和随机取m个特征。</p>
<h3 id="分类效果">分类效果</h3>
<ul>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>
<p>　　减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<h3 id="袋外错误率out-of-bag-error">袋外错误率（out of bag error）</h3>
<p>袋外错误率主要用于选取最佳的m值。</p>
<p>对每个样本，使用没有选择这个样本的决策树进行预测，使用少数服从多数的方法，得到预测结果。</p>
<p>最终得到每个样本是否被正确预测，计算袋外错误率。</p>
<h3 id="代码使用">代码使用</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">100</span>)<span class="comment">#树的数量</span></span><br></pre></td></tr></table></figure>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/373879791">决策树之分类树与回归树 - 知乎 (zhihu.com)</a></p>
<p>[<a href="https://zhuanlan.zhihu.com/p/406627649">机器学习基础复习] 随机森林(Random Forest) - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/Daycym/article/details/84455299">(12条消息) 【机器学习】分类决策树与回归决策树案例_Daycym的博客-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/cpc784221489/article/details/92085702">(12条消息) 随机森林算法详解_阿斯达克-CSDN博客_随机森林详解</a></p>
<p>https://www.zhihu.com/question/24904495/answer/371618173</p>
]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
</search>
